---
layout: post
title:  "Reverse mode Automatic Differentiation"
tags: tensorflow ANN algorithms
---
TensorFlow uses a computational graph because it uses a
Reverse mode Automatic Differentiation algorithm.

The article
["Introduction TensorFlow - Optimization of Objective Functions", Wieschollek, 2017](http://patwie.com/tutorials/tensorflow-optimization.html)
describes the gradients from `tf.gradient` are "neither computed by finite differences, nor in a symbolic fashion."
TensorFlow uses "reverse-mode auto-differentiation" - or RevAD.

The article
["Reverse-mode automatic differentiation: a tutorial", Rufflewind, 2016](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation)
explains RevAD in more detail. I'd say that RevAD uses symbolic differentiation.
But there is a computational element in the handling of the noisy input data.
The article notes there are several techniques for implementing RevAD
but focuses on a computational graph implemention.
This implementation closely describes the computational graph structure used by TensorFlow.
Further information is available from this paper:
[â€œAutomatic differentiation of algorithms", Bartholomew-Biggs, 2000](https://doi.org/10.1016/S0377-0427%2800%2900422-2)
